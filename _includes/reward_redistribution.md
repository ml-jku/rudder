Reward redistribution is achieved by using a function $$g$$ which predicts the return $$G$$ at the end of the episode from the state action sequence and then using contribution analysis on $$g$$. Contribution analysis computes the contribution of current input to the final output, that is the information gain by the current input on the final prediction. We use the individual contributions as the new reward in the new MDP. We show that this redistribution, as it is *return-equivalent* does not change the optimal policies.  
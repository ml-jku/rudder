For reward redistribution we assume an MDP with one reward (=return) at sequence end which can be predicted from the last state-action pair. We want to remove this Markov property to enforce the LSTM to store the relevant past events that caused the reward. This way, contribution analysis will identify these relevant past events and give reward to them. To eliminate this Markov property, we define a difference $$\Delta$$ between a state-action pair and its predecessor, where the  $$\Delta$$s are assumed to be mutually independent from each other. Now, the reward at sequence end cannot be predicted from the last $$\Delta$$. The return can still be predicted from the sequence of $$\Delta$$s. Since the  $$\Delta$$s are mutually independent, the contribution of each $$\Delta$$ to the return must be stored in the hidden states of the LSTM to predict the final reward. The $$\Delta$$ can be generic since states and actions can be numbered and then the difference of this numbers can be used for $$\Delta$$. In the applications like Atari with immediate rewards we give the accumulated reward at the end of the episode without enriching the states. This has a similar effect as using $$\Delta$$. We force the LSTM to build up an internal state which tracks the already accumulated reward.
Example: The agent has to take a key to open the door. Since it is an MDP, the agent is always aware to have the key indicated by a bit to be on. The reward can be predicted in the last step. Using differences $$\Delta$$ the bit is zero, except for the step where the agent takes the key. Thus, the LSTM has to store this event and transfers reward to it.